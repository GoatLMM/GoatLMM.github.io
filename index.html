<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>GOAT-Bench</title>
    <link rel="icon" type="image/x-icon" href="static/images/goat.jpg">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title"> GOAT-Bench:<br/> Safety Insights to Large Multimodal Models through Meme-Based Social Abuse</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=hOF1SLoAAAAJ&hl=en" target="_blank">Hongzhan Lin</a><sup>*</sup>,</span>

                            <span class="author-block"><a href="https://chiyeunglaw.github.io/" target="_blank">Ziyang Luo</a><sup>*</sup>,</span>

                            <span class="author-block"><a href="https://scholar.google.com/citations?user=AwFj_u8AAAAJ&hl=en" target="_blank">Bo Wang</a><sup></sup>,</span>

                            <span class="author-block"><a href="https://scholar.google.com/citations?user=DI3rqUAAAAAJ&hl=en"
                                target="_blank">Ruichao Yang</a><sup></sup>,</span>

                            <span class="author-block"><a href="https://majingcuhk.github.io/"
                                    target="_blank">Jing Ma</a><sup>‡</sup></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup></sup>Hong Kong Baptist University</span>
                            <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution, <sup>‡</sup>Corresponding
                                    author</small></span>
                            <span class="eql-cntrb"><small><br><a
										href="mailto:cshzlin@comp.hkbu.edu.hk">cshzlin@comp.hkbu.edu.hk</a>, <a
                                        href="mailto:cszyluo@comp.hkbu.edu.hk">cszyluo@comp.hkbu.edu.hk</a>, <a
                                        href="mailto:majing@comp.hkbu.edu.hk">majing@comp.hkbu.edu.hk</a></small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2401.01523" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/HKBU-NLP/GOAT-Bench" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            &#x1F917;
                                        </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                               
                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="?" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (coming soon)</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness, misogyny, offensiveness, sarcasm, and harmful content. Our extensive experiments across a range of LMMs reveal that current models still exhibit a deficiency in safety awareness, showing insensitivity to various forms of implicit abuse. We posit that this shortfall represents a critical impediment to the realization of safe artificial intelligence.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">The GOAT Benchmark</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/goat-bench.png" alt="goatbench" />
                            <div class="content has-text-justified">
				<p>
                                We introduce the GOAT-Bench, a comprehensive and specialized dataset designed to evaluate large multimodal models through meme-based multimodal social abuse. GOAT-Bench comprises over 6K diverse memes, encompassing a range of themes including hate speech and offensive content. Our focus is to assess the ability of LMMs to accurately identify online abuse, specifically in terms of hatefulness, misogyny, offensiveness, sarcasm, and harmfulness. We meticulously control for the granularity of each specific meme task to facilitate a detailed analysis. Furthermore, we extend our evaluation to assess the effectiveness of thought chains in discerning the underlying implications of memes for deducing their potential threat to safety.
                            	</p>
			</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
	
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop has-text-centered">
                <h2 class="title is-3">Experiment Results</h2>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/radar.png" alt="radar" />
                        </div>
                    </div>
                </div>
		<div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/table1.png" alt="table1" />
                        </div>
                    </div>
                </div>
		<div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Your image here -->
                            <img style="width: 100%;" src="static/images/table2.png" alt="table2" />
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="hero"> -->
        <!-- <div class="hero-body"> -->
            <!-- <div class="container is-max-desktop"> -->
                <!-- <h2 class="title is-3 has-text-centered">RLEIF</h2> -->
                <!-- <div class="columns is-centered"> -->
                    <!-- <div class="column is-four-fifths"> -->
                        <!-- <div class="item"> -->
                            <!-- <article class="message is-dark"> -->
                                <!-- <div class="message-header"> -->
                                    <!-- <p>Answer Augmentation (AnsAug)</p> -->
                                <!-- </div> -->
                                <!-- <div class="message-body"> -->
                                    <!-- <p> -->
                                        <!-- <b>Generated Answer:</b> -->
                                        <!-- James buys 5 packs of beef that are 4 pounds each, -->
                                        <!-- so he buys a total of 5 * 4 = 20 pounds of beef. The price of beef is $5.50 -->
                                        <!-- per pound, so he pays 20 * $5.50 = $110. The answer is: 110 -->
                                    <!-- </p> -->
                                <!-- </div> -->
                            <!-- </article> -->
                            <!-- <article class="message is-dark"> -->
                                <!-- <div class="message-header"> -->
                                    <!-- <p>Rephrasing Question</p> -->
                                <!-- </div> -->
                                <!-- <div class="message-body"> -->
                                    <!-- <p> -->
                                        <!-- <b>Question:</b> What is the total amount that James paid when he purchased 5 packs of beef, each weighing 4 -->
                                        <!-- pounds, at a price of $5.50 per pound? -->
                                    <!-- </p> -->
                                    <!-- <p> -->
                                        <!-- <b>Rephrase the above question:</b> Each pack of beef weighs 4 pounds, so 5 packs weigh 4 * 5 = 20 pounds in total. The price per -->
                                        <!-- pound of beef is $5.50, so the total cost for 20 pounds is 20 * $5.50 = $110. Therefore, James paid a total of -->
                                        <!-- $110. The answer is: 110. -->
                                    <!-- </p> -->
                                <!-- </div> -->
                            <!-- </article> -->
                            <!-- <article class="message is-dark"> -->
                                <!-- <div class="message-header"> -->
                                    <!-- <p>Self-Verification Question</p> -->
                                <!-- </div> -->
                                <!-- <div class="message-body"> -->
                                    <!-- <p><b>Question:</b> James buys <span style="color: red">x</span> packs of beef that -->
                                        <!-- are -->
                                        <!-- 4 pounds each. The price of beef is \$5.50 per pound. He paid 110. <span -->
                                            <!-- style="color: red">What is the value of unknown variable x?</span></p> -->
                                    <!-- <p> -->
                                        <!-- <b>Answer:</b> To solve this problem, we need to determine the value of x, which -->
                                        <!-- represents the number of packs of beef that James bought. -->
                                        <!-- Each pack of beef weighs 4 pounds and costs $5.50 per pound. -->
                                        <!-- The total amount James paid is $110. -->
                                        <!-- We can set up the equation as follows: -->
                                        <!-- Number of packs of beef * Weight per pack * Price per pound = Total amount paid; -->
                                        <!-- x * 4 * $5.50 = $110; -->
                                        <!-- Let's simplify and solve for x: -->
                                        <!-- 4x * $5.50 = $110; -->
                                        <!-- 22x = $110; -->
                                        <!-- To isolate x, we divide both sides of the equation by 22: -->
                                        <!-- 22x / 22 = $110 / 22; -->
                                        <!-- x = $5; -->
                                        <!-- The value of x is 5. -->

                                    <!-- </p> -->
                                <!-- </div> -->
                            <!-- </article> -->
                            <!-- <article class="message is-dark"> -->
                                <!-- <div class="message-header"> -->
                                    <!-- <p>FOBAR Question</p> -->
                                <!-- </div> -->
                                <!-- <div class="message-body"> -->
                                    <!-- <p> -->
                                        <!-- <b>Question:</b> -->
                                        <!-- James buys <span style="color: red">x</span> packs of beef that are 4 pounds -->
                                        <!-- each. -->
                                        <!-- The price of beef is $5.50 per pound. How much did he pay? -->
                                        <!-- <span style="color: red">If we know the answer to the above question is 110, -->
                                            <!-- what is -->
                                            <!-- the value of unknown variable x?</span> -->
                                    <!-- </p> -->
                                    <!-- <p> -->
                                        <!-- <b>Answer:</b> -->
                                        <!-- James buys x packs of beef that are 4 pounds each, so he buys a total of 4x -->
                                        <!-- pounds -->
                                        <!-- of beef. -->
                                        <!-- The price of beef is $5.50 per pound, so the total cost of the beef is 5.50 * 4x -->
                                        <!-- = -->
                                        <!-- 22x. -->
                                        <!-- We are given that the total cost is $110, so we can write: 22x = 110. -->
                                        <!-- Dividing both sides by 22, we get: x = 5. -->
                                        <!-- The value of x is 5. -->
                                    <!-- </p> -->
                                <!-- </div> -->
                            <!-- </article> -->
                        <!-- </div> -->
                    <!-- </div> -->
                <!-- </div> -->
            <!-- </div> -->
        <!-- </div> -->
    <!-- </section> -->

    <!-- <section class="hero"> -->
        <!-- <div class="hero-body"> -->
            <!-- <div class="container is-max-desktop"> -->
                <!-- <div class="columns is-centered"> -->
                    <!-- <div class="column has-text-centered is-fifths-fifths"> -->
                        <!-- <h2 class="title is-3">Experiment Results</h2> -->
                        <!-- <div class="content has-text-justified"> -->
                            <!-- <table> -->
                                <!-- <thead> -->
                                    <!-- <tr> -->
                                        <!-- <th>Model</th> -->
                                        <!-- <th>#params</th> -->
                                        <!-- <th>GSM8K</th> -->
                                        <!-- <th>MATH</th> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                    <!-- </tr> -->
                                <!-- </thead> -->
                                <!-- <tbody id="tabResults"> -->
                                    <!-- <tr class="th"> -->
                                        <!-- <td colspan="4" style="text-align: center; font-weight: bold;"> -->
                                            <!-- Closed-source Model</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>GPT-4</td> -->
                                        <!-- <td>-</td> -->
                                        <!-- <td>92.0</td> -->
                                        <!-- <td>42.5</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>GPT-3.5-Turbo</td> -->
                                        <!-- <td>-</td> -->
                                        <!-- <td>80.8</td> -->
                                        <!-- <td>34.1</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>PaLM</td> -->
                                        <!-- <td>8B</td> -->
                                        <!-- <td>4.1</td> -->
                                        <!-- <td>1.5</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>PaLM</td> -->
                                        <!-- <td>62B</td> -->
                                        <!-- <td>33.0</td> -->
                                        <!-- <td>4.4</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>PaLM</td> -->
                                        <!-- <td>540B</td> -->
                                        <!-- <td>56.5</td> -->
                                        <!-- <td>8.8</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>PaLM-2</td> -->
                                        <!-- <td>540B</td> -->
                                        <!-- <td>80.7</td> -->
                                        <!-- <td>34.3</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Flan-PaLM 2</td> -->
                                        <!-- <td>540B</td> -->
                                        <!-- <td>84.7</td> -->
                                        <!-- <td>33.2</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Minerva</td> -->
                                        <!-- <td>8B</td> -->
                                        <!-- <td>16.2</td> -->
                                        <!-- <td>14.1</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Minerva</td> -->
                                        <!-- <td>62B</td> -->
                                        <!-- <td>52.4</td> -->
                                        <!-- <td>27.6</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Minerva</td> -->
                                        <!-- <td>540B</td> -->
                                        <!-- <td>58.8</td> -->
                                        <!-- <td>33.6</td> -->
                                    <!-- </tr> -->

                                    <!-- <tr class="th"> -->
                                        <!-- <td colspan="4" style="text-align: center; font-weight: bold;"> -->
                                            <!-- Open-source models (1-10B)</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-1</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>11.0</td> -->
                                        <!-- <td>2.9</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-2</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>14.6</td> -->
                                        <!-- <td>2.5</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>MPT</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>6.8</td> -->
                                        <!-- <td>3.0</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Falcon</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>6.8</td> -->
                                        <!-- <td>2.3</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>InternLM</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>31.2</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>GPT-J</td> -->
                                        <!-- <td>6B</td> -->
                                        <!-- <td>34.9</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>ChatGLM 2</td> -->
                                        <!-- <td>6B</td> -->
                                        <!-- <td>32.4</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Qwen</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>51.6</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Baichuan-2</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>24.5</td> -->
                                        <!-- <td>5.6</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>SFT</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>41.6</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>RFT</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>50.3</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>WizardMath</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td>54.9</td> -->
                                        <!-- <td>10.7</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>MetaMath (<b>ours</b>)</td> -->
                                        <!-- <td>7B</td> -->
                                        <!-- <td><b>66.5</b></td> -->
                                        <!-- <td><b>19.8</b></td> -->
                                    <!-- </tr> -->
                                    <!-- <tr class="th"> -->
                                        <!-- <td colspan="4" style="text-align: center; font-weight: bold;"> -->
                                            <!-- Open-source models (11-50B)</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-1</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>17.8</td> -->
                                        <!-- <td>3.9</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-1</td> -->
                                        <!-- <td>33B</td> -->
                                        <!-- <td>35.6</td> -->
                                        <!-- <td>7.1</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-2</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>28.7</td> -->
                                        <!-- <td>3.9</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-2</td> -->
                                        <!-- <td>34B</td> -->
                                        <!-- <td>42.2</td> -->
                                        <!-- <td>6.2</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>MPT</td> -->
                                        <!-- <td>30B</td> -->
                                        <!-- <td>15.2</td> -->
                                        <!-- <td>3.1</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Falcon</td> -->
                                        <!-- <td>40B</td> -->
                                        <!-- <td>19.6</td> -->
                                        <!-- <td>2.5</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>GAL</td> -->
                                        <!-- <td>30B</td> -->
                                        <!-- <td>-</td> -->
                                        <!-- <td>12.7</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Vicuna</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>27.6</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>Baichuan-2</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>52.8</td> -->
                                        <!-- <td>10.1</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>SFT</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>50.0</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>RFT</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>54.8</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>WizardMath</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td>63.9</td> -->
                                        <!-- <td>14.0</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>MetaMath (<b>ours</b>)</td> -->
                                        <!-- <td>13B</td> -->
                                        <!-- <td><b>72.3</b></td> -->
                                        <!-- <td><b>22.4</b></td> -->
                                    <!-- </tr> -->

                                    <!-- <tr class="th"> -->
                                        <!-- <td colspan="4" style="text-align: center; font-weight: bold;"> -->
                                            <!-- Open-source models (50-70B)</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-1</td> -->
                                        <!-- <td>65B</td> -->
                                        <!-- <td>50.9</td> -->
                                        <!-- <td>10.6</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>LLaMA-2</td> -->
                                        <!-- <td>70B</td> -->
                                        <!-- <td>56.8</td> -->
                                        <!-- <td>13.5</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>RFT</td> -->
                                        <!-- <td>70B</td> -->
                                        <!-- <td>64.8</td> -->
                                        <!-- <td>-</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>WizardMath</td> -->
                                        <!-- <td>70B</td> -->
                                        <!-- <td>81.6</td> -->
                                        <!-- <td>22.7</td> -->
                                    <!-- </tr> -->
                                    <!-- <tr> -->
                                        <!-- <td>MetaMath (<b>ours</b>) <sup>‡</sup></td> -->
                                        <!-- <td>70B</td> -->
                                        <!-- <td><b>82.3</b></td> -->
                                        <!-- <td><b>26.6</b></td> -->
                                    <!-- </tr> -->
                        <!-- </div> -->
                        <!-- </tbody> -->
                        <!-- </table> -->
                    <!-- </div> -->
                    <!-- <h2 class="subtitle">Table 1: Comparison of testing accuracy to existing LLMs on GSM8K and MATH. -->
                        <!-- <sup>‡</sup>Due to the computing resource limitation, we finetune MetaMath-70B using QLoRA. -->
                    <!-- </h2> -->
                <!-- </div> -->
            <!-- </div> -->
        <!-- </div> -->
        <!-- </div> -->
    <!-- </section> -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{lin2024goatbench,
                title={GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse}, 
                author={Hongzhan Lin and Ziyang Luo and Bo Wang and Ruichao Yang and Jing Ma},
                year={2024},
                eprint={2401.01523},
                archivePrefix={arXiv},
                primaryClass={cs.CL}
          }</code></pre>
        </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
